# OREILY_트랜스포머를 활용한 자연어 처리

## 기본 설정

#### 1. 가상환경 생성 
 * 본인 conda 사용
   - conda create -n 가상환경이름
   - activate 가상환경이름
#### 2. requirement 설치 
 * pip install -r requirements.txt
#### 3. 가상환경 커널 이름 설정 ( 주피터를 사용시 설정 )
 * python -m ipykernel install --user --name conda --display-name "주피터에 표시할 kernel 이름" 
#### 4. 토큰이나 api key를 심어둘 env 파일 생성
 * .env 파일 생성 (깃에 올릴 폴더 말고 다른 폴더에 저장해야 공유 막음)

# 1장 트랜스포머 소개

트랜스포머는 자연어 처리와 기타 시퀀스 학습 작업에 사용되는 딥러닝 아키텍처 중 하나입니다. 이는 Attention is All You Need 라는 논문에서 처음 소개되었고, 이 아키텍처는 RNN이나 CNN과 달리 self-attention 매커니즘을 기반으로 합니다. 이를 제대로 설명하기에 앞서 이 아키텍처가 나오기까지의 과정에 대해 먼저 간략하게 설명 드리겠습니다.    
. 참고로 해당 책에선 이론에 대해선 간략하게 짚고 넘어가므로(추가로 생략한 내용도 있음) 이론에 대해서 추가 학습을 위해선 다른 책 또는 유튜브,강의 등을 참고하시기 바랍니다.

### 1-1. 인코더-디코더 프레임워크  
-  이 구조는 입력 시퀀스를 표현하는 인코더와 해당 표현을 기반으로 출력 시퀀스를 생성하는 디코더로 구성됩니다.  
- 인코더는 입력 시퀀스를 임베딩하여 고정 길이의 벡터 표현으로 변환합니다. 이러한 벡터 표현은 일반적으로 RNN(순환 신경망)이나 CNN(합성곱 신경망)과 같은 순차적인 모델을 사용하여 생성됩니다. 인코더의 목표는 입력 시퀀스의 의미를 적절히 보존하는 벡터 표현을 만드는 것입니다.  
-  디코더는 인코더에서 생성된 벡터 표현을 기반으로 출력 시퀀스를 생성합니다. 이를 위해 디코더는 인코더와 유사한 모델을 사용하여 시퀀스를 생성하는 과정을 수행합니다. 디코더는 이전에 생성된 토큰들을 입력으로 사용하여 다음 토큰을 예측하는 과정을 반복하여 출력 시퀀스를 생성합니다.  

인코더의 압축된 정보(상태)가 디코더에 넘어갈때 마지막 hidden state에서 정보 병목이 발생됩니다. 시퀀스가 길때 모든 정보를 하나의 hidden state에 담기 어렵기 때문입니다. 그래서 이를 해결해줄 메커니즘으로 어텐션이 나오게 됩니다.

### 1-2. 어텐션 메커니즘
어텐션은 입력시퀀스에서 은닉상태를 만들지 않고 스텝마다 인코더에서 디코더가 참고할 은닉 상태를 출력한다는 개념에 기초한 메커니즘입니다. 하지만 모든 입력 시퀀스가 동등하게 하나의 출력 시퀀스에 영향을 주는 것이 아니니 가중치를 할당합니다.   
하지만 인코더,디코더 내에서 순환 모델이 사용되므로 병목 현상이 남아있고, 계산이 순차적으로 수행되므로 병렬화가 어렵습니다. 그래서 순환을 모두 없애고 인코더,디코더 내에서도 어텐션을 사용 함(셀프 어텐션)으로써 문제를 해결한 트랜스포머가 나오게 됩니다.

### 1-4. 허깅페이스 트랜스포머
트랜스포머로 만들어진 모델들을 초기엔 모델 아키텍처인 파이토치나 텐서플로로 구현을 하고, 보통 서버로부터 사전 훈련된 가중치를 가져와서 인풋을 전처리하여 모델에 전달합니다. 그리고 손실함수나 옵티마이저를 조정하여 결과를 만들어내는데, 허깅페이스에서 이를 라이브러리와 허브로 구성하여 손쉽게 사용 할 수 있도록 정리하였습니다. 라이브러리는 코드를 제공하고, 허브는 사전 훈련된 모델 가중치, 데이터셋, 평가 지표를 위한 스크립트를 제공합니다.

이것을 잘 활용한다면 다양한 모델을 간단히 실험하고 우리는 프로젝트에서 특정 도메인에 해당하는 부분에 대해 더욱 집중 할 수 있습니다.
